{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f1d1dc09-9e90-481e-9844-98a7b38e7be7",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Analysis of the Top Cities and Companies for Data Science Jobs Across Germany\"\n",
    "description: \"We map the jobs across Germany and give the reader the chance to explore them through an interactive map and table\"\n",
    "author: Flavia\n",
    "date: 2025-06-15\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true  # optional: adds a show/hide all toggle\n",
    "categories: [job analysis, python, geodata, data visualization, data science, maps]\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668edd9-8e2d-4e08-83eb-cda64709b7eb",
   "metadata": {},
   "source": [
    "Here we build an interactive map and html table of data science jobs across Germany.\n",
    "We are using one of the job posting datasets that we collect weekly by using the [Adzuna API](https://developer.adzuna.com/) through **GitHub Actions**. The full script is available on my [GitHub Repository](https://github.com/Flazoukie/adzuna-scraper-cleaned).\n",
    "We collected data science-related job postings in Germany by filtering:\n",
    "\n",
    "* Keyword = \"data science\"\n",
    "* country or region (e.g., \"de\")\n",
    "* pages = 5\n",
    "* results_per_page = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de019d4-bf0f-46e0-b67f-16122f5bfd1b",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4611932f-b25d-485b-b43c-d7f6917c7029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Core Libraries\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import time\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# üî¢ Data Handling & Math\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# üìä Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# üó∫Ô∏è Mapping, Geocoding & Clustering\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from sklearn.cluster import DBSCAN\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from folium import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ae8d2a-40ac-4e96-acdd-d2feecc80ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to save the file for today's analysis\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "output_dir = f\"output/{today}\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e545269f-f03b-45d4-ac1f-6811c27e753e",
   "metadata": {},
   "source": [
    "## üìÅ Get the Data\n",
    "\n",
    "We load the last data from Adzuna in the following steps:\n",
    "\n",
    "* Search for all CSV files in the data/ folder whose filenames start with \"jobs_\", e.g., \"jobs_2024-06-01.csv\".\n",
    "* Sort them in reverse (latest first).\n",
    "* Load the most recent one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using latest file: data\\jobs_2025-06-30.csv\n",
      "Loaded DataFrame with 250 rows and 17 columns\n"
     ]
    }
   ],
   "source": [
    "#print(\"Current directory:\", os.getcwd())\n",
    "#print(\"Files in data folder:\", os.listdir(\"data\"))\n",
    "\n",
    "data_files = sorted(glob(\"data/jobs_*.csv\"), reverse=True)\n",
    "\n",
    "if data_files:\n",
    "    latest_file = data_files[0]\n",
    "    print(f\"Using latest file: {latest_file}\")\n",
    "    \n",
    "    try:\n",
    "        # Add defensive options\n",
    "        df = pd.read_csv(latest_file, encoding='utf-8', engine='python', on_bad_lines='skip')\n",
    "        print(f\"Loaded DataFrame with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV: {e}\")\n",
    "        with open(latest_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            print(\"First 10 lines of file:\")\n",
    "            print(''.join(lines[:10]))\n",
    "        raise\n",
    "else:\n",
    "    raise FileNotFoundError(\"No CSV files found in the data/ directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e53ea7-8b46-4ba8-903a-1b62566d3698",
   "metadata": {},
   "source": [
    "## üßπ Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f98963-2e67-4ebb-be60-a12f9fc44e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning\n",
    "\n",
    "# function to clean up city names later\n",
    "def normalize_city_name(city):\n",
    "    if pd.isna(city):\n",
    "        return city\n",
    "    return re.sub(r\"\\s*\\(.*?\\)\", \"\", city).strip()\n",
    "\n",
    "    \n",
    "# extract display name from the location (contains city and state)\n",
    "def safe_extract_display_name(val):\n",
    "    try:\n",
    "        parsed = ast.literal_eval(val)\n",
    "        return parsed.get(\"display_name\")\n",
    "    except:\n",
    "        return val  # If not a dict string, keep the raw string like \"Dresden\"\n",
    "\n",
    "df[\"location\"] = df[\"location\"].apply(lambda x: safe_extract_display_name(x) if pd.notna(x) else None)\n",
    "df[\"company\"] = df[\"company\"].apply(lambda x: safe_extract_display_name(x) if pd.notna(x) else None)\n",
    "df[\"title\"] = df[\"title\"].fillna(\"Unknown\")\n",
    "\n",
    "# drop duplicates based on job title, company, and location\n",
    "df = df.drop_duplicates(subset=[\"title\", \"company\", \"location\"])\n",
    "\n",
    "# Keep only needed columns\n",
    "cols_to_keep = [\n",
    "    \"id\", \"title\", \"company\", \"location\", \"latitude\", \"longitude\", \n",
    "    \"created\", \"redirect_url\"\n",
    "]\n",
    "df = df[cols_to_keep].copy()\n",
    "\n",
    "# Assign unique ID for merging later\n",
    "df = df.reset_index(drop=True)\n",
    "df[\"job_id\"] = df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d754bd5b-ae45-4831-9ee7-b2a8674cfdd7",
   "metadata": {},
   "source": [
    "## üåé Get City Names by Using the Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183cd4f6-3865-4a20-844c-3d827ede5ec6",
   "metadata": {},
   "source": [
    "Location descriptions can sometimes be imprecise or contain typos, or refer to smaller districts rather than the main city (for example, ‚ÄúBilk‚Äù instead of ‚ÄúD√ºsseldorf‚Äù), which can cause errors in visualization. Instead of extracting all cities from the location description, we decided to use the coordinate provided in the dataset (when available) or calculate them.\n",
    "\n",
    "For entries with coordinates:\n",
    "* We perform **reverse geocoding** on the coordinates to find the corresponding city names.\n",
    "* To avoid redundant API calls and speed up future runs, we implement a **local cache** stored in a JSON file.\n",
    "\n",
    "For entries missing latitude or longitude:\n",
    "* We attempt to extract a city name from a string-based location field\n",
    "* We then geocode these city names to recover their latitude and longitude coordinates.\n",
    "* These recovered coordinates are merged back into the main DataFrame to fill missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f1db78-11cc-41a6-9e34-b93b666ca51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geocode city name from longitude and latitude and build chache\n",
    "\n",
    "# üß≠ Setup geolocator\n",
    "geolocator = Nominatim(user_agent=\"adzuna-geocoder\")\n",
    "\n",
    "# Load cache file if exists, else create empty dict\n",
    "CACHE_FILE = \"coord_to_city_cache.json\"\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    with open(CACHE_FILE, \"r\") as f:\n",
    "        coord_to_city = json.load(f)\n",
    "else:\n",
    "    coord_to_city = {}\n",
    "\n",
    "def get_city_from_coords(lat, lon, pause=1.1):\n",
    "    key = f\"{lat},{lon}\"\n",
    "    if key in coord_to_city:\n",
    "        return coord_to_city[key]\n",
    "\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), exactly_one=True, language=\"en\")\n",
    "        if location is None:\n",
    "            city = None\n",
    "        else:\n",
    "            address = location.raw.get(\"address\", {})\n",
    "            city = (\n",
    "                address.get(\"city\") or \n",
    "                address.get(\"town\") or \n",
    "                address.get(\"village\") or \n",
    "                address.get(\"municipality\") or \n",
    "                address.get(\"county\") or \n",
    "                address.get(\"state\")\n",
    "            )\n",
    "        coord_to_city[key] = city\n",
    "        time.sleep(pause)\n",
    "        return city\n",
    "    except Exception as e:\n",
    "        print(f\"Error reverse geocoding {lat}, {lon}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Prepare unique coordinates from df\n",
    "df_coords = df.dropna(subset=[\"latitude\", \"longitude\"]).copy()\n",
    "unique_coords = df_coords[[\"latitude\", \"longitude\"]].drop_duplicates()\n",
    "\n",
    "# Reverse geocode with caching\n",
    "for _, row in unique_coords.iterrows():\n",
    "    lat, lon = row[\"latitude\"], row[\"longitude\"]\n",
    "    get_city_from_coords(lat, lon)\n",
    "\n",
    "# Assign city names back to df_coords using the cache\n",
    "df_coords[\"city\"] = df_coords.apply(\n",
    "    lambda r: coord_to_city.get(f\"{r['latitude']},{r['longitude']}\"), axis=1\n",
    ")\n",
    "\n",
    "# normalize city names\n",
    "df_coords[\"city\"] = df_coords[\"city\"].apply(normalize_city_name)\n",
    "\n",
    "# Save cache to disk for next runs\n",
    "with open(CACHE_FILE, \"w\") as f:\n",
    "    json.dump(coord_to_city, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a396e585-6ece-429c-934a-8c7f5e076685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß≠ Geocode missing coordinates from location string\n",
    "\n",
    "def extract_city_from_location(location):\n",
    "    if pd.isna(location):\n",
    "        return None\n",
    "    if isinstance(location, str):\n",
    "        try:\n",
    "            location_dict = ast.literal_eval(location)\n",
    "            area = location_dict.get(\"area\", [])\n",
    "            if isinstance(area, list) and area:\n",
    "                return area[-1]\n",
    "        except Exception:\n",
    "            pass\n",
    "    match = re.search(r\"\\b([A-Z√Ñ√ñ√ú][a-z√§√∂√º√üA-Z√Ñ√ñ√ú-]+)\", location)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "missing_coords = df[df[\"latitude\"].isna() | df[\"longitude\"].isna()].copy()\n",
    "missing_coords[\"recovered_city\"] = missing_coords[\"location\"].apply(extract_city_from_location)\n",
    "\n",
    "# normalize city names\n",
    "missing_coords[\"recovered_city\"] = missing_coords[\"recovered_city\"].apply(normalize_city_name)\n",
    "\n",
    "\n",
    "# Filter out entries that are too broad\n",
    "valid_city_mask = missing_coords[\"recovered_city\"].notna() & (missing_coords[\"recovered_city\"].str.lower() != \"deutschland\")\n",
    "to_geocode = missing_coords[valid_city_mask].copy()\n",
    "\n",
    "def geocode_city(city):\n",
    "    try:\n",
    "        if pd.isna(city):\n",
    "            return (None, None)\n",
    "        location = geolocator.geocode(f\"{city}, Germany\")\n",
    "        if location:\n",
    "            return (location.latitude, location.longitude)\n",
    "    except GeocoderTimedOut:\n",
    "        time.sleep(1)\n",
    "        return geocode_city(city)  # retry once\n",
    "    except Exception:\n",
    "        return (None, None)\n",
    "    return (None, None)\n",
    "\n",
    "coords_df = to_geocode[\"recovered_city\"].apply(lambda city: pd.Series(geocode_city(city)))\n",
    "coords_df.columns = [\"latitude_recovered\", \"longitude_recovered\"]\n",
    "\n",
    "to_geocode = to_geocode.reset_index(drop=True)\n",
    "to_geocode = pd.concat([to_geocode, coords_df], axis=1)\n",
    "to_geocode[\"job_id\"] = to_geocode.index\n",
    "\n",
    "# Drop previously added columns if they exist (avoid merge errors)\n",
    "df = df.drop(columns=[\"latitude_recovered\", \"longitude_recovered\"], errors=\"ignore\")\n",
    "\n",
    "# Merge recovered coordinates back into the main DataFrame\n",
    "df = df.merge(to_geocode[[\"job_id\", \"latitude_recovered\", \"longitude_recovered\"]], on=\"job_id\", how=\"left\")\n",
    "\n",
    "# Fill missing lat/lon with recovered values\n",
    "df[\"latitude\"] = df[\"latitude\"].fillna(df[\"latitude_recovered\"])\n",
    "df[\"longitude\"] = df[\"longitude\"].fillna(df[\"longitude_recovered\"])\n",
    "\n",
    "# Final cleanup\n",
    "df = df.drop(columns=[\"latitude_recovered\", \"longitude_recovered\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea97102-73ea-47d4-9d26-017e8afa52d9",
   "metadata": {},
   "source": [
    "## üîù Top Cities for Job Offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c062841-527c-45cc-a103-d1af41515bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top 20 Cities for Job Offers\n",
    "\n",
    "top_cities = df_coords[\"city\"].value_counts().head(20).reset_index() # get top 20\n",
    "top_cities.columns = [\"city\", \"count\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(data=top_cities, x=\"city\", y=\"count\", palette=\"Blues_d\", hue=\"city\", legend=False)\n",
    "\n",
    "# Add annotations\n",
    "for i, row in top_cities.iterrows():\n",
    "    ax.text(i, row[\"count\"], str(row[\"count\"]), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.title(\"Top Cities by Number of Data Science Job Offers\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"City\")\n",
    "plt.ylabel(\"Number of Job Offers\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ‚úÖ Save BEFORE show\n",
    "plt.savefig(f\"{output_dir}/top_cities.png\", bbox_inches='tight')\n",
    "\n",
    "# show\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7ae2ab-d108-4771-8ac6-6d8bd0764175",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Mapping Job Clusters and Top Hiring Companies in Germany\n",
    "To visualize where data science jobs are concentrated across Germany, we used the geographical coordinates provided in the dataset (latitude and longitude) and applied the algorithm **DBSCAN** to create clusters. This allowed us to group nearby job postings within a 10 km radius into location-based clusters.\n",
    "\n",
    "For each cluster, we:\n",
    "\n",
    "* Calculated the average coordinates to place a marker on the map.\n",
    "* Counted the number of job postings and identified the top 10 hiring companies in each cluster.\n",
    "\n",
    "We then used the **Folium library** to create an interactive map. Each marker on the map displays:\n",
    "\n",
    "* The name of the city (based on the reverse-geocoded coordinates)\n",
    "* The top companies hiring in that location\n",
    "* The number of job postings per company (e.g., ‚ÄúCompany XYZ (3)‚Äù)\n",
    "\n",
    "This approach helps identify regional hiring trends and hotspots for data science roles in Germany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad3813-16fe-462d-a1cc-4f09dcb54152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DBSCAN clustering\n",
    "\n",
    "# since DBSCAN needs the eps in radians, not kilometers and Earth's radius is approximately 6371 km\n",
    "# we divide 10 km by that value to convert it to radians.\n",
    "\n",
    "# Run DBSCAN clustering\n",
    "coords = df_coords[['latitude', 'longitude']].values\n",
    "kms_per_radian = 6371.0088\n",
    "epsilon = 10 / kms_per_radian  # 10 km radius in radians\n",
    "\n",
    "db = DBSCAN(eps=epsilon, min_samples=1, algorithm='ball_tree', metric='haversine')\n",
    "df_coords['cluster'] = db.fit_predict(np.radians(coords))\n",
    "\n",
    "# Assign representative city name to each cluster (city with most jobs)\n",
    "cluster_names = (\n",
    "    df_coords.groupby('cluster')['city']\n",
    "    .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else x.iloc[0])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_coords = df_coords.merge(cluster_names, on='cluster', suffixes=('', '_clustered'))\n",
    "df_coords['city'] = df_coords['city_clustered']\n",
    "df_coords.drop(columns='city_clustered', inplace=True)\n",
    "df_coords = df_coords.drop(columns='cluster_city', errors='ignore')  # if it existed\n",
    "\n",
    "# Summarize clusters with mean coordinates and job counts\n",
    "cluster_summary = (\n",
    "    df_coords.groupby(['cluster', 'city'])\n",
    "    .agg({\n",
    "        'latitude': 'mean',\n",
    "        'longitude': 'mean',\n",
    "        'title': 'count'\n",
    "    })\n",
    "    .reset_index()\n",
    "    .rename(columns={'title': 'job_count'})\n",
    ")\n",
    "\n",
    "# Prepare top companies per cluster (for popup info)\n",
    "top_companies_per_cluster = (\n",
    "    df_coords.groupby(['cluster', 'company'])\n",
    "    .size()\n",
    "    .reset_index(name='job_count')\n",
    "    .sort_values(['cluster', 'job_count'], ascending=[True, False])\n",
    "    .groupby('cluster', group_keys=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "popup_data = top_companies_per_cluster.merge(\n",
    "    cluster_summary[['cluster', 'city', 'latitude', 'longitude']],\n",
    "    on='cluster'\n",
    ")\n",
    "\n",
    "# Create map with company popups\n",
    "map_center = [51.1657, 10.4515]  # Germany center\n",
    "company_map = folium.Map(location=map_center, zoom_start=6)\n",
    "marker_cluster = MarkerCluster().add_to(company_map)\n",
    "\n",
    "for cluster_id, group in popup_data.groupby('cluster'):\n",
    "    city = group['city'].iloc[0]\n",
    "    lat = group['latitude'].iloc[0]\n",
    "    lon = group['longitude'].iloc[0]\n",
    "\n",
    "    companies_html = '<br>'.join(\n",
    "        f\"{row['company']} ({row['job_count']})\" for _, row in group.iterrows()\n",
    "    )\n",
    "    html = f\"\"\"\n",
    "    <h4>{city}</h4>\n",
    "    <div style=\"font-family: Arial; font-size: 12px;\">\n",
    "    {companies_html}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    iframe = IFrame(html=html, width=300, height=150)\n",
    "    popup = folium.Popup(iframe, max_width=300)\n",
    "\n",
    "    folium.Marker(\n",
    "        location=[lat, lon],\n",
    "        popup=popup,\n",
    "        tooltip=city\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Save the interactive map as an HTML file (make sure folder exists)\n",
    "company_map.save(f\"{output_dir}/company_map.html\")\n",
    "\n",
    "# Optional: display the note in the notebook (commented out to keep notebook light)\n",
    "# display(HTML(\"\"\"\n",
    "# <p style=\"font-family: Arial; font-size: 14px; margin-top: 10px;\">\n",
    "# <b>Note:</b> The numbers on the map markers represent how many job locations are clustered together in that area.\n",
    "# Zoom in to explore individual job locations and click on them to view the top hiring companies in that region\n",
    "# and the number of jobs per each company. The city names are in English!\n",
    "# </p>\n",
    "# \"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d80ff4-f40f-46b3-8299-e4be5955fa58",
   "metadata": {},
   "source": [
    "## üìä Interactive Company & Job Title Chart per City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346228d7-7e2d-48ff-b85b-8d6755cc110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate job data\n",
    "table_data = (\n",
    "    df_coords\n",
    "    .groupby(['city', 'company'])  \n",
    "    .agg(\n",
    "        job_titles=('title', list),\n",
    "        job_links=('redirect_url', list),\n",
    "        job_dates=('created', list),\n",
    "        latest_date=('created', lambda dates: max(pd.to_datetime(dates)))  # For sorting\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "# Format job titles + links + dates into HTML and sort by published date\n",
    "def format_job_entry(title, link, date):\n",
    "    raw_date = pd.to_datetime(date)\n",
    "    display_date = raw_date.strftime('%d-%m-%Y')  # EU format\n",
    "    return (\n",
    "        f\"{title}<br>\"\n",
    "        f\"<a href='{link}' target='_blank'>üîó Link</a><br>\"\n",
    "        f\"<em>date: {display_date}</em>\"\n",
    "    )\n",
    "\n",
    "table_data['jobs'] = table_data.apply(\n",
    "    lambda row: '<br><br>'.join(\n",
    "        format_job_entry(t, l, d)\n",
    "        for t, l, d in zip(row['job_titles'], row['job_links'], row['job_dates'])\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 3: Final table with renamed columns (include SortDate as hidden column)\n",
    "table_data_display = table_data[['city', 'company', 'jobs', 'latest_date']].rename(columns={\n",
    "    'city': 'City',\n",
    "    'company': 'Company',\n",
    "    'jobs': 'Job Offers',\n",
    "    'latest_date': 'SortDate'\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fix date format for sorting\n",
    "table_data['latest_date'] = pd.to_datetime(table_data['latest_date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Final table with sorting column\n",
    "table_data_display = table_data[['city', 'company', 'jobs', 'latest_date']].rename(columns={\n",
    "    'city': 'City',\n",
    "    'company': 'Company',\n",
    "    'jobs': 'Job Offers',\n",
    "    'latest_date': 'SortDate'\n",
    "})\n",
    "\n",
    "# HTML table\n",
    "html_table = table_data_display.to_html(\n",
    "    escape=False, index=False, classes='display', table_id='jobTable'\n",
    ")\n",
    "\n",
    "# HTML Template\n",
    "html_template = f\"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<meta charset=\"utf-8\">\n",
    "<title>Job Offers Interactive Table</title>\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.datatables.net/1.13.6/css/jquery.dataTables.css\">\n",
    "<script src=\"https://code.jquery.com/jquery-3.7.0.js\"></script>\n",
    "<script src=\"https://cdn.datatables.net/1.13.6/js/jquery.dataTables.js\"></script>\n",
    "<style>\n",
    "  body {{\n",
    "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "    background-color: #fafafa;\n",
    "    padding: 20px;\n",
    "  }}\n",
    "\n",
    "  h2 {{\n",
    "    color: #333;\n",
    "  }}\n",
    "\n",
    "  p {{\n",
    "    margin-bottom: 10px;\n",
    "    font-size: 0.95em;\n",
    "    color: #555;\n",
    "  }}\n",
    "\n",
    "  #cityFilter {{\n",
    "    margin-bottom: 20px;\n",
    "    font-size: 1em;\n",
    "  }}\n",
    "\n",
    "  table.dataTable {{\n",
    "    border-collapse: collapse !important;\n",
    "    width: 100%;\n",
    "    background-color: white;\n",
    "    overflow: hidden;\n",
    "    box-shadow: 0 2px 10px rgba(0,0,0,0.05);\n",
    "  }}\n",
    "\n",
    "  table.dataTable thead {{\n",
    "    background-color: #333;\n",
    "    color: white;\n",
    "  }}\n",
    "\n",
    "  table.dataTable th,\n",
    "  table.dataTable td {{\n",
    "    padding: 10px;\n",
    "    text-align: left;\n",
    "    vertical-align: top;\n",
    "    white-space: normal;\n",
    "  }}\n",
    "\n",
    "  table.dataTable tbody tr:nth-child(odd) {{\n",
    "    background-color: #f9f9f9;\n",
    "  }}\n",
    "\n",
    "  table.dataTable tbody tr:hover {{\n",
    "    background-color: #eef2f7;\n",
    "    transition: background-color 0.2s ease;\n",
    "  }}\n",
    "\n",
    "  a {{\n",
    "    color: #1a73e8;\n",
    "    text-decoration: none;\n",
    "  }}\n",
    "\n",
    "  a:hover {{\n",
    "    text-decoration: underline;\n",
    "  }}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<h2>Data Science Job Offers by City and Company</h2>\n",
    "<p>Use the dropdown to filter by city or the search box to filter by keywords. Click the üîó link to view or apply for a job.</p>\n",
    "\n",
    "<label for=\"cityFilter\"><strong>Filter by City:</strong></label>\n",
    "<select id=\"cityFilter\">\n",
    "  <option value=\"\">All Cities</option>\n",
    "</select>\n",
    "\n",
    "{html_table}\n",
    "\n",
    "<script>\n",
    "$(document).ready(function() {{\n",
    "    var table = $('#jobTable').DataTable({{\n",
    "        \"pageLength\": 10,\n",
    "        \"lengthMenu\": [5, 10, 20, 50],\n",
    "        \"order\": [[3, 'desc']],\n",
    "        \"columnDefs\": [\n",
    "            {{\n",
    "                \"targets\": 3,\n",
    "                \"visible\": false,\n",
    "                \"searchable\": false\n",
    "            }}\n",
    "        ]\n",
    "    }});\n",
    "\n",
    "    // Populate City filter dropdown\n",
    "    var uniqueCities = table.column(0).data().unique().sort();\n",
    "    uniqueCities.toArray().forEach(function(city) {{\n",
    "        $('#cityFilter').append(`<option value=\"${{city}}\">${{city}}</option>`);\n",
    "    }});\n",
    "\n",
    "    // Filter table by selected city\n",
    "    $('#cityFilter').on('change', function() {{\n",
    "        var selected = $(this).val();\n",
    "        if (selected) {{\n",
    "            table.column(0).search('^' + selected + '$', true, false).draw();\n",
    "        }} else {{\n",
    "            table.column(0).search('').draw();\n",
    "        }}\n",
    "    }});\n",
    "}});\n",
    "</script>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6219850-a7bd-48d3-9237-703c7b659537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save interactive table for allowing display on my quarto blog\n",
    "with open(f\"{output_dir}/interactive_job_table.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0de6c-d61b-4a2f-b44e-863509f7616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "# display\n",
    "#IFrame(f\"{output_dir}/interactive_job_table.html\", width='90%', height=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
