{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f1d1dc09-9e90-481e-9844-98a7b38e7be7",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Analysis of the Top Cities and Companies for Data Science Jobs Across Germany\"\n",
    "description: \"We map the jobs across Germany and give the reader the chance to explore them through an interactive map and table\"\n",
    "author: Flavia\n",
    "date: 2025-06-15\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true  # optional: adds a show/hide all toggle\n",
    "categories: [job analysis, python, geodata, data visualization, data science, maps]\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668edd9-8e2d-4e08-83eb-cda64709b7eb",
   "metadata": {},
   "source": [
    "Here we build an interactive map and html table of data science jobs across Germany.\n",
    "We are using one of the job posting datasets that we collect weekly by using the [Adzuna API](https://developer.adzuna.com/) through **GitHub Actions**. The full script is available on my [GitHub Repository](https://github.com/Flazoukie/adzuna-scraper-cleaned).\n",
    "We collected data science-related job postings in Germany by filtering:\n",
    "\n",
    "* Keyword = \"data science\"\n",
    "* country or region (e.g., \"de\")\n",
    "* pages = 5\n",
    "* results_per_page = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de019d4-bf0f-46e0-b67f-16122f5bfd1b",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4611932f-b25d-485b-b43c-d7f6917c7029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Core Libraries\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import time\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# 🔢 Data Handling & Math\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 📊 Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# 🗺️ Mapping, Geocoding & Clustering\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from sklearn.cluster import DBSCAN\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from folium import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6ae8d2a-40ac-4e96-acdd-d2feecc80ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to save the file for today's analysis\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "output_dir = f\"output/{today}\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e545269f-f03b-45d4-ac1f-6811c27e753e",
   "metadata": {},
   "source": [
    "## 📁 Get the Data\n",
    "\n",
    "We load the last data from Adzuna in the following steps:\n",
    "\n",
    "* Search for all CSV files in the data/ folder whose filenames start with \"jobs_\", e.g., \"jobs_2024-06-01.csv\".\n",
    "* Sort them in reverse (latest first).\n",
    "* Load the most recent one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Users\\flavi\\PycharmProjects\\adzuna-scraper-cleaned\n",
      "Files in data folder: ['.gitkeep', 'jobs_2025-06-12.csv', 'jobs_2025-06-16.csv', 'jobs_2025-06-17.csv', 'jobs_2025-06-18.csv']\n",
      "Using latest file: data\\jobs_2025-06-18.csv\n",
      "Error reading CSV: No columns to parse from file\n",
      "First 10 lines of file:\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStopIteration\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\adzuna-scraper-cleaned\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:404\u001b[39m, in \u001b[36mPythonParser._infer_columns\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m     line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffered_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.line_pos <= hr:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\adzuna-scraper-cleaned\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:637\u001b[39m, in \u001b[36mPythonParser._buffered_line\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    636\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\adzuna-scraper-cleaned\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:738\u001b[39m, in \u001b[36mPythonParser._next_line\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m     orig_line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_iter_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_num\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m     \u001b[38;5;28mself\u001b[39m.pos += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\adzuna-scraper-cleaned\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:805\u001b[39m, in \u001b[36mPythonParser._next_iter_line\u001b[39m\u001b[34m(self, row_num)\u001b[39m\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m805\u001b[39m line = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[38;5;66;03m# for mypy\u001b[39;00m\n",
      "\u001b[31mStopIteration\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mEmptyDataError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing latest file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatest_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Add defensive options\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatest_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpython\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_bad_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mskip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded DataFrame with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columns\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\adzuna-scraper-cleaned\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\adzuna-scraper-cleaned\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\adzuna-scraper-cleaned\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\adzuna-scraper-cleaned\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\adzuna-scraper-cleaned\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:133\u001b[39m, in \u001b[36mPythonParser.__init__\u001b[39m\u001b[34m(self, f, **kwds)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28mself\u001b[39m._col_indices: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    128\u001b[39m columns: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar | \u001b[38;5;28;01mNone\u001b[39;00m]]\n\u001b[32m    129\u001b[39m (\n\u001b[32m    130\u001b[39m     columns,\n\u001b[32m    131\u001b[39m     \u001b[38;5;28mself\u001b[39m.num_original_columns,\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mself\u001b[39m.unnamed_cols,\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_infer_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# Now self.columns has the set of columns that we will process.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# The original set is stored in self.original_columns.\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'index_names'\u001b[39;00m\n\u001b[32m    138\u001b[39m (\n\u001b[32m    139\u001b[39m     \u001b[38;5;28mself\u001b[39m.columns,\n\u001b[32m    140\u001b[39m     \u001b[38;5;28mself\u001b[39m.index_names,\n\u001b[32m   (...)\u001b[39m\u001b[32m    145\u001b[39m     \u001b[38;5;28mself\u001b[39m.index_names,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m    146\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\adzuna-scraper-cleaned\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:432\u001b[39m, in \u001b[36mPythonParser._infer_columns\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    429\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m columns, num_original_columns, unnamed_cols\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.names:\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m EmptyDataError(\u001b[33m\"\u001b[39m\u001b[33mNo columns to parse from file\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    434\u001b[39m     line = \u001b[38;5;28mself\u001b[39m.names[:]\n\u001b[32m    436\u001b[39m this_columns: \u001b[38;5;28mlist\u001b[39m[Scalar | \u001b[38;5;28;01mNone\u001b[39;00m] = []\n",
      "\u001b[31mEmptyDataError\u001b[39m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "print(\"Current directory:\", os.getcwd())\n",
    "print(\"Files in data folder:\", os.listdir(\"data\"))\n",
    "\n",
    "data_files = sorted(glob(\"data/jobs_*.csv\"), reverse=True)\n",
    "\n",
    "if data_files:\n",
    "    latest_file = data_files[0]\n",
    "    print(f\"Using latest file: {latest_file}\")\n",
    "    \n",
    "    try:\n",
    "        # Add defensive options\n",
    "        df = pd.read_csv(latest_file, encoding='utf-8', engine='python', on_bad_lines='skip')\n",
    "        print(f\"Loaded DataFrame with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV: {e}\")\n",
    "        with open(latest_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            print(\"First 10 lines of file:\")\n",
    "            print(''.join(lines[:10]))\n",
    "        raise\n",
    "else:\n",
    "    raise FileNotFoundError(\"No CSV files found in the data/ directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e53ea7-8b46-4ba8-903a-1b62566d3698",
   "metadata": {},
   "source": [
    "## 🧹 Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f98963-2e67-4ebb-be60-a12f9fc44e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning\n",
    "\n",
    "# extract display name from the location (contains city and state)\n",
    "def safe_extract_display_name(val):\n",
    "    try:\n",
    "        parsed = ast.literal_eval(val)\n",
    "        return parsed.get(\"display_name\")\n",
    "    except:\n",
    "        return val  # If not a dict string, keep the raw string like \"Dresden\"\n",
    "\n",
    "df[\"location\"] = df[\"location\"].apply(lambda x: safe_extract_display_name(x) if pd.notna(x) else None)\n",
    "df[\"company\"] = df[\"company\"].apply(lambda x: safe_extract_display_name(x) if pd.notna(x) else None)\n",
    "df[\"title\"] = df[\"title\"].fillna(\"Unknown\")\n",
    "\n",
    "# drop duplicates based on job title, company, and location\n",
    "df = df.drop_duplicates(subset=[\"title\", \"company\", \"location\"])\n",
    "\n",
    "# Keep only needed columns\n",
    "cols_to_keep = [\n",
    "    \"id\", \"title\", \"company\", \"location\", \"latitude\", \"longitude\", \n",
    "    \"created\", \"redirect_url\"\n",
    "]\n",
    "df = df[cols_to_keep].copy()\n",
    "\n",
    "# Assign unique ID for merging later\n",
    "df = df.reset_index(drop=True)\n",
    "df[\"job_id\"] = df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d754bd5b-ae45-4831-9ee7-b2a8674cfdd7",
   "metadata": {},
   "source": [
    "## 🌎 Get City Names by Using the Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183cd4f6-3865-4a20-844c-3d827ede5ec6",
   "metadata": {},
   "source": [
    "Location descriptions can sometimes be imprecise or contain typos, or refer to smaller districts rather than the main city (for example, “Bilk” instead of “Düsseldorf”), which can cause errors in visualization. Instead of extracting all cities from the location description, we decided to use the coordinate provided in the dataset (when available) or calculate them.\n",
    "\n",
    "For entries with coordinates:\n",
    "* We perform **reverse geocoding** on the coordinates to find the corresponding city names.\n",
    "* To avoid redundant API calls and speed up future runs, we implement a **local cache** stored in a JSON file.\n",
    "\n",
    "For entries missing latitude or longitude:\n",
    "* We attempt to extract a city name from a string-based location field\n",
    "* We then geocode these city names to recover their latitude and longitude coordinates.\n",
    "* These recovered coordinates are merged back into the main DataFrame to fill missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f1db78-11cc-41a6-9e34-b93b666ca51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geocode city name from longitude and latitude and build chache\n",
    "\n",
    "# 🧭 Setup geolocator\n",
    "geolocator = Nominatim(user_agent=\"adzuna-geocoder\")\n",
    "\n",
    "# Load cache file if exists, else create empty dict\n",
    "CACHE_FILE = \"coord_to_city_cache.json\"\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    with open(CACHE_FILE, \"r\") as f:\n",
    "        coord_to_city = json.load(f)\n",
    "else:\n",
    "    coord_to_city = {}\n",
    "\n",
    "def get_city_from_coords(lat, lon, pause=1.1):\n",
    "    key = f\"{lat},{lon}\"\n",
    "    if key in coord_to_city:\n",
    "        return coord_to_city[key]\n",
    "\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), exactly_one=True, language=\"en\")\n",
    "        if location is None:\n",
    "            city = None\n",
    "        else:\n",
    "            address = location.raw.get(\"address\", {})\n",
    "            city = (\n",
    "                address.get(\"city\") or \n",
    "                address.get(\"town\") or \n",
    "                address.get(\"village\") or \n",
    "                address.get(\"municipality\") or \n",
    "                address.get(\"county\") or \n",
    "                address.get(\"state\")\n",
    "            )\n",
    "        coord_to_city[key] = city\n",
    "        time.sleep(pause)\n",
    "        return city\n",
    "    except Exception as e:\n",
    "        print(f\"Error reverse geocoding {lat}, {lon}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Prepare unique coordinates from df\n",
    "df_coords = df.dropna(subset=[\"latitude\", \"longitude\"]).copy()\n",
    "unique_coords = df_coords[[\"latitude\", \"longitude\"]].drop_duplicates()\n",
    "\n",
    "# Reverse geocode with caching\n",
    "for _, row in unique_coords.iterrows():\n",
    "    lat, lon = row[\"latitude\"], row[\"longitude\"]\n",
    "    get_city_from_coords(lat, lon)\n",
    "\n",
    "# Assign city names back to df_coords using the cache\n",
    "df_coords[\"city\"] = df_coords.apply(\n",
    "    lambda r: coord_to_city.get(f\"{r['latitude']},{r['longitude']}\"), axis=1\n",
    ")\n",
    "\n",
    "# Save cache to disk for next runs\n",
    "with open(CACHE_FILE, \"w\") as f:\n",
    "    json.dump(coord_to_city, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a396e585-6ece-429c-934a-8c7f5e076685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧭 Geocode missing coordinates from location string\n",
    "\n",
    "def extract_city_from_location(location):\n",
    "    if pd.isna(location):\n",
    "        return None\n",
    "    if isinstance(location, str):\n",
    "        try:\n",
    "            location_dict = ast.literal_eval(location)\n",
    "            area = location_dict.get(\"area\", [])\n",
    "            if isinstance(area, list) and area:\n",
    "                return area[-1]\n",
    "        except Exception:\n",
    "            pass\n",
    "    match = re.search(r\"\\b([A-ZÄÖÜ][a-zäöüßA-ZÄÖÜ-]+)\", location)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "missing_coords = df[df[\"latitude\"].isna() | df[\"longitude\"].isna()].copy()\n",
    "missing_coords[\"recovered_city\"] = missing_coords[\"location\"].apply(extract_city_from_location)\n",
    "\n",
    "# Filter out entries that are too broad\n",
    "valid_city_mask = missing_coords[\"recovered_city\"].notna() & (missing_coords[\"recovered_city\"].str.lower() != \"deutschland\")\n",
    "to_geocode = missing_coords[valid_city_mask].copy()\n",
    "\n",
    "def geocode_city(city):\n",
    "    try:\n",
    "        if pd.isna(city):\n",
    "            return (None, None)\n",
    "        location = geolocator.geocode(f\"{city}, Germany\")\n",
    "        if location:\n",
    "            return (location.latitude, location.longitude)\n",
    "    except GeocoderTimedOut:\n",
    "        time.sleep(1)\n",
    "        return geocode_city(city)  # retry once\n",
    "    except Exception:\n",
    "        return (None, None)\n",
    "    return (None, None)\n",
    "\n",
    "coords_df = to_geocode[\"recovered_city\"].apply(lambda city: pd.Series(geocode_city(city)))\n",
    "coords_df.columns = [\"latitude_recovered\", \"longitude_recovered\"]\n",
    "\n",
    "to_geocode = to_geocode.reset_index(drop=True)\n",
    "to_geocode = pd.concat([to_geocode, coords_df], axis=1)\n",
    "to_geocode[\"job_id\"] = to_geocode.index\n",
    "\n",
    "# Drop previously added columns if they exist (avoid merge errors)\n",
    "df = df.drop(columns=[\"latitude_recovered\", \"longitude_recovered\"], errors=\"ignore\")\n",
    "\n",
    "# Merge recovered coordinates back into the main DataFrame\n",
    "df = df.merge(to_geocode[[\"job_id\", \"latitude_recovered\", \"longitude_recovered\"]], on=\"job_id\", how=\"left\")\n",
    "\n",
    "# Fill missing lat/lon with recovered values\n",
    "df[\"latitude\"] = df[\"latitude\"].fillna(df[\"latitude_recovered\"])\n",
    "df[\"longitude\"] = df[\"longitude\"].fillna(df[\"longitude_recovered\"])\n",
    "\n",
    "# Final cleanup\n",
    "df = df.drop(columns=[\"latitude_recovered\", \"longitude_recovered\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea97102-73ea-47d4-9d26-017e8afa52d9",
   "metadata": {},
   "source": [
    "## 🔝 Top Cities for Job Offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c062841-527c-45cc-a103-d1af41515bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top 20 Cities for Job Offers\n",
    "\n",
    "top_cities = df_coords[\"city\"].value_counts().head(20).reset_index() # get top 20\n",
    "top_cities.columns = [\"city\", \"count\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(data=top_cities, x=\"city\", y=\"count\", palette=\"Blues_d\", hue=\"city\", legend=False)\n",
    "\n",
    "# Add annotations\n",
    "for i, row in top_cities.iterrows():\n",
    "    ax.text(i, row[\"count\"], str(row[\"count\"]), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.title(\"Top Cities by Number of Data Science Job Offers\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"City\")\n",
    "plt.ylabel(\"Number of Job Offers\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9767a4-dfb2-41c9-90ff-d478ffc68797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output\n",
    "plt.savefig(f\"{output_dir}/top_cities.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7ae2ab-d108-4771-8ac6-6d8bd0764175",
   "metadata": {},
   "source": [
    "## 🗺️ Mapping Job Clusters and Top Hiring Companies in Germany\n",
    "To visualize where data science jobs are concentrated across Germany, we used the geographical coordinates provided in the dataset (latitude and longitude) and applied the algorithm **DBSCAN** to create clusters. This allowed us to group nearby job postings within a 10 km radius into location-based clusters.\n",
    "\n",
    "For each cluster, we:\n",
    "\n",
    "* Calculated the average coordinates to place a marker on the map.\n",
    "* Counted the number of job postings and identified the top 10 hiring companies in each cluster.\n",
    "\n",
    "We then used the **Folium library** to create an interactive map. Each marker on the map displays:\n",
    "\n",
    "* The name of the city (based on the reverse-geocoded coordinates)\n",
    "* The top companies hiring in that location\n",
    "* The number of job postings per company (e.g., “Company XYZ (3)”)\n",
    "\n",
    "This approach helps identify regional hiring trends and hotspots for data science roles in Germany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad3813-16fe-462d-a1cc-4f09dcb54152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DBSCAN clustering\n",
    "\n",
    "# since DBSCAN needs the eps in radians, not kilometers and Earth's radius is approximately 6371 km\n",
    "# we divide 10 km by that value to convert it to radians.\n",
    "coords = df_coords[['latitude', 'longitude']].values\n",
    "kms_per_radian = 6371.0088\n",
    "epsilon = 10 / kms_per_radian  # 10 km radius\n",
    "\n",
    "# initialize DBSCAN clustering algorithm:\n",
    "db = DBSCAN(eps=epsilon, min_samples=1, algorithm='ball_tree', metric='haversine')\n",
    "df_coords['cluster'] = db.fit_predict(np.radians(coords))\n",
    "\n",
    "# Assign representative city name to each cluster (city with most jobs)\n",
    "cluster_names = (\n",
    "    df_coords.groupby('cluster')['city']\n",
    "    .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else x.iloc[0])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_coords = df_coords.merge(cluster_names, on='cluster', suffixes=('', '_clustered'))\n",
    "df_coords['city'] = df_coords['city_clustered']\n",
    "df_coords.drop(columns='city_clustered', inplace=True)\n",
    "\n",
    "df_coords = df_coords.drop(columns='cluster_city', errors='ignore')  # if it existed\n",
    "\n",
    "# Summarize clusters with mean coordinates and job counts\n",
    "cluster_summary = (\n",
    "    df_coords.groupby(['cluster', 'city'])\n",
    "    .agg({\n",
    "        'latitude': 'mean',\n",
    "        'longitude': 'mean',\n",
    "        'title': 'count'\n",
    "    })\n",
    "    .reset_index()\n",
    "    .rename(columns={'title': 'job_count'})\n",
    ")\n",
    "\n",
    "\n",
    "# Prepare top companies per cluster (will popup when clicking)\n",
    "top_companies_per_cluster = (\n",
    "    df_coords.groupby(['cluster', 'company'])\n",
    "    .size()\n",
    "    .reset_index(name='job_count')\n",
    "    .sort_values(['cluster', 'job_count'], ascending=[True, False])\n",
    "    .groupby('cluster', group_keys=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "popup_data = top_companies_per_cluster.merge(\n",
    "    cluster_summary[['cluster', 'city', 'latitude', 'longitude']],\n",
    "    on='cluster'\n",
    ")\n",
    "\n",
    "\n",
    "# Create map with company popups\n",
    "map_center = [51.1657, 10.4515]  # Germany center\n",
    "company_map = folium.Map(location=map_center, zoom_start=6)\n",
    "marker_cluster = MarkerCluster().add_to(company_map)\n",
    "\n",
    "for cluster_id, group in popup_data.groupby('cluster'):\n",
    "    city = group['city'].iloc[0]\n",
    "    lat = group['latitude'].iloc[0]\n",
    "    lon = group['longitude'].iloc[0]\n",
    "\n",
    "    companies_html = '<br>'.join(\n",
    "    f\"{row['company']} ({row['job_count']})\" for _, row in group.iterrows()\n",
    "    )\n",
    "    html = f\"\"\"\n",
    "    <h4>{city}</h4>\n",
    "    <div style=\"font-family: Arial; font-size: 12px;\">\n",
    "    {companies_html}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    iframe = IFrame(html=html, width=300, height=150)\n",
    "    popup = folium.Popup(iframe, max_width=300)\n",
    "\n",
    "    folium.Marker(\n",
    "        location=[lat, lon],\n",
    "        popup=popup,\n",
    "        tooltip=city\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "display(company_map)\n",
    "\n",
    "\n",
    "# Add map note for reader\n",
    "display(HTML(\"\"\"\n",
    "<p style=\"font-family: Arial; font-size: 14px; margin-top: 10px;\">\n",
    "<b>Note:</b> The numbers on the map markers represent how many job locations are clustered together in that area.\n",
    "Zoom in to explore individual job locations and click on them to view the top hiring companies in that region\n",
    "and the number of jobs per each company. The city names are in English!\n",
    "</p>\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b06fbb-facf-4ac0-a8a8-a61b1c1cb980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save map\n",
    "company_map.save(f\"{output_dir}/company_map.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d80ff4-f40f-46b3-8299-e4be5955fa58",
   "metadata": {},
   "source": [
    "## 📊 Interactive Company & Job Title Chart per City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346228d7-7e2d-48ff-b85b-8d6755cc110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate job data\n",
    "table_data = (\n",
    "    df_coords\n",
    "    .groupby(['city', 'company'])  \n",
    "    .agg(\n",
    "        job_titles=('title', list),\n",
    "        job_links=('redirect_url', list),\n",
    "        job_dates=('created', list),\n",
    "        latest_date=('created', lambda dates: max(pd.to_datetime(dates)))  # For sorting\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "# Format job titles + links + dates into HTML and sort by published date\n",
    "def format_job_entry(title, link, date):\n",
    "    raw_date = pd.to_datetime(date)\n",
    "    display_date = raw_date.strftime('%d-%m-%Y')  # EU format\n",
    "    return (\n",
    "        f\"{title}<br>\"\n",
    "        f\"<a href='{link}' target='_blank'>🔗 Link</a><br>\"\n",
    "        f\"<em>date: {display_date}</em>\"\n",
    "    )\n",
    "\n",
    "table_data['jobs'] = table_data.apply(\n",
    "    lambda row: '<br><br>'.join(\n",
    "        format_job_entry(t, l, d)\n",
    "        for t, l, d in zip(row['job_titles'], row['job_links'], row['job_dates'])\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 3: Final table with renamed columns (include SortDate as hidden column)\n",
    "table_data_display = table_data[['city', 'company', 'jobs', 'latest_date']].rename(columns={\n",
    "    'city': 'City',\n",
    "    'company': 'Company',\n",
    "    'jobs': 'Job Offers',\n",
    "    'latest_date': 'SortDate'\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fix date format for sorting\n",
    "table_data['latest_date'] = pd.to_datetime(table_data['latest_date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Final table with sorting column\n",
    "table_data_display = table_data[['city', 'company', 'jobs', 'latest_date']].rename(columns={\n",
    "    'city': 'City',\n",
    "    'company': 'Company',\n",
    "    'jobs': 'Job Offers',\n",
    "    'latest_date': 'SortDate'\n",
    "})\n",
    "\n",
    "# HTML table\n",
    "html_table = table_data_display.to_html(\n",
    "    escape=False, index=False, classes='display', table_id='jobTable'\n",
    ")\n",
    "\n",
    "# HTML Template\n",
    "html_template = f\"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<meta charset=\"utf-8\">\n",
    "<title>Job Offers Interactive Table</title>\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.datatables.net/1.13.6/css/jquery.dataTables.css\">\n",
    "<script src=\"https://code.jquery.com/jquery-3.7.0.js\"></script>\n",
    "<script src=\"https://cdn.datatables.net/1.13.6/js/jquery.dataTables.js\"></script>\n",
    "<style>\n",
    "  body {{\n",
    "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "    background-color: #fafafa;\n",
    "    padding: 20px;\n",
    "  }}\n",
    "\n",
    "  h2 {{\n",
    "    color: #333;\n",
    "  }}\n",
    "\n",
    "  p {{\n",
    "    margin-bottom: 10px;\n",
    "    font-size: 0.95em;\n",
    "    color: #555;\n",
    "  }}\n",
    "\n",
    "  #cityFilter {{\n",
    "    margin-bottom: 20px;\n",
    "    font-size: 1em;\n",
    "  }}\n",
    "\n",
    "  table.dataTable {{\n",
    "    border-collapse: collapse !important;\n",
    "    width: 100%;\n",
    "    background-color: white;\n",
    "    overflow: hidden;\n",
    "    box-shadow: 0 2px 10px rgba(0,0,0,0.05);\n",
    "  }}\n",
    "\n",
    "  table.dataTable thead {{\n",
    "    background-color: #333;\n",
    "    color: white;\n",
    "  }}\n",
    "\n",
    "  table.dataTable th,\n",
    "  table.dataTable td {{\n",
    "    padding: 10px;\n",
    "    text-align: left;\n",
    "    vertical-align: top;\n",
    "    white-space: normal;\n",
    "  }}\n",
    "\n",
    "  table.dataTable tbody tr:nth-child(odd) {{\n",
    "    background-color: #f9f9f9;\n",
    "  }}\n",
    "\n",
    "  table.dataTable tbody tr:hover {{\n",
    "    background-color: #eef2f7;\n",
    "    transition: background-color 0.2s ease;\n",
    "  }}\n",
    "\n",
    "  a {{\n",
    "    color: #1a73e8;\n",
    "    text-decoration: none;\n",
    "  }}\n",
    "\n",
    "  a:hover {{\n",
    "    text-decoration: underline;\n",
    "  }}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<h2>Data Science Job Offers by City and Company</h2>\n",
    "<p>Use the dropdown to filter by city or the search box to filter by keywords. Click the 🔗 link to view or apply for a job.</p>\n",
    "\n",
    "<label for=\"cityFilter\"><strong>Filter by City:</strong></label>\n",
    "<select id=\"cityFilter\">\n",
    "  <option value=\"\">All Cities</option>\n",
    "</select>\n",
    "\n",
    "{html_table}\n",
    "\n",
    "<script>\n",
    "$(document).ready(function() {{\n",
    "    var table = $('#jobTable').DataTable({{\n",
    "        \"pageLength\": 10,\n",
    "        \"lengthMenu\": [5, 10, 20, 50],\n",
    "        \"order\": [[3, 'desc']],\n",
    "        \"columnDefs\": [\n",
    "            {{\n",
    "                \"targets\": 3,\n",
    "                \"visible\": false,\n",
    "                \"searchable\": false\n",
    "            }}\n",
    "        ]\n",
    "    }});\n",
    "\n",
    "    // Populate City filter dropdown\n",
    "    var uniqueCities = table.column(0).data().unique().sort();\n",
    "    uniqueCities.toArray().forEach(function(city) {{\n",
    "        $('#cityFilter').append(`<option value=\"${{city}}\">${{city}}</option>`);\n",
    "    }});\n",
    "\n",
    "    // Filter table by selected city\n",
    "    $('#cityFilter').on('change', function() {{\n",
    "        var selected = $(this).val();\n",
    "        if (selected) {{\n",
    "            table.column(0).search('^' + selected + '$', true, false).draw();\n",
    "        }} else {{\n",
    "            table.column(0).search('').draw();\n",
    "        }}\n",
    "    }});\n",
    "}});\n",
    "</script>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6219850-a7bd-48d3-9237-703c7b659537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save interactive table for allowing display on my quarto blog\n",
    "with open(f\"{output_dir}/interactive_job_table.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0de6c-d61b-4a2f-b44e-863509f7616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "# display\n",
    "IFrame('output/interactive_job_table.html', width='90%', height=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
