name: Update Job Visuals and Push to Blog

on:
  schedule:
    - cron: '0 8 * * 1'  # Every Monday at 08:00 UTC
  workflow_dispatch:    # Allows manual trigger

jobs:
  update-blog:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout scraper repo with LFS
      uses: actions/checkout@v4
      with:
        lfs: true  # ðŸ‘ˆ This enables downloading LFS-tracked files like your CSV

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'  # Use stable version

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-ci.txt

    - name: Run scraper
      run: python adzuna_scraper.py

    - name: Verify CSV file
      run: |
        echo "Checking contents of data directory:"
        ls -lh data/
        echo "Preview of CSV file:"
        head -n 10 data/jobs_*.csv || echo "No data to preview"
        echo "Sleeping to ensure file write completes..."
        sleep 3

    - name: Run notebook to generate visuals
      run: |
        pip install jupyter pandas matplotlib seaborn folium
        jupyter nbconvert --execute analyze_jobs.ipynb --to notebook --inplace

    - name: Clone blog repo
      run: |
        git config --global user.name "github-actions"
        git config --global user.email "actions@github.com"
        git clone https://x-access-token:${{ secrets.BLOG_REPO_TOKEN }}@github.com/Flazoukie/data-blog.git blog

    - name: Copy output to blog
      run: |
        mkdir -p blog/posts/latest-jobs
        cp output/* blog/posts/latest-jobs/

    - name: Commit and push to blog repo
      run: |
        cd blog
        git add posts/latest-jobs/
        git commit -m "Update latest job visualizations" || echo "No changes to commit"
        git push


